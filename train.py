import h5py

import data
import numpy as np
import model as m
import argparse


'''
This module provides all the methods needed to train and validation test a deep neural network. Training includes:
1. Data Extraction or Loading already extracted data
2. Preprocessing:
    a. Mean subtraction + pixel scaling + storing of dataset mean for use in deployment.
    b. Support for large datasets via HDF5 real-time sampling during training. The sample is randomized for training set
    and deterministic for validation set. By choosing a percentage of your training set of say 0.2 you will get 20% of
    your data at random for each epoch, so if you have 100 000 images you will have in memory 20 000 of them, which is
    feasible given today's standard desktops of 16-32 GBs of ram. Any more than that and you'll get out of memory errors
3. Data Augmentation: To increase the robustness of the model against rotated images, mirrored images and in general
to improve the robustness against variability of image poses angles we have implemented data augmented training,
which allows one to randomly rotate and mirror their images in between epochs. This simple technique has proven
invaluable to modern machine learning. Not only does it improve robustness but provides additional samples in the
form of randomly edited original images that helps prevent overfitting, produces a higher accuracy/lower loss and as
a result improves generalization performance.
4. Automatic checkpointing (saving of models) after each epoch (named latest_model_weights). And automatic selection of
best model and saving of it's weights named best_model_weights. As well as support for resuming training from the last
checkpoint using the "cont" argument in the run method. Just run python train.py run continue and you should be up and
running.
5. Validation checks after each epoch, as well as output of top-k accuracy for your convenience.
'''



def get_top_n_error(preds, y, n):
    index_of_true = np.argmax(y, axis=1)
    index_of_preds = np.argsort(preds, axis=1)
    total = float(len(y))
    correct = float(0)

    for i in range(len(index_of_true)):
        for j in range(1, n + 1):
            if index_of_true[i] == index_of_preds[i, -j]:
                correct = correct + 1
                break

    accuracy = float(correct / total)

    return accuracy



def run(epochs=500, training_percentage=0.4, validation_percentage=0.1, extract=True, cont=True, size=256, top_k=5):
    '''Does the routine required to get the data, put them in needed format and start training the model
       saves weights whenever the model produces a better test result and keeps track of the best loss'''
    if extract:
        print("Extracting data..")
        X, y = data.extract_data(size=size)

        print("Preprocessing data..")
        X, y, nb_samples, num_categories = data.preprocess_data(X, y, save=True, subtract_mean=True)

    else:
        print("Loading data..")
        h5f = h5py.File('data.hdf5', 'r')
        nb_samples = h5f['nb_samples'].value
        num_categories = h5f['n_categories'].value
        h5f.close()

    print("Number of categories: {}".format(num_categories))
    print("Number of samples {}".format(nb_samples))

    data_ids = np.arange(start=0, stop=nb_samples)
    val_ids = data.produce_validation_indices(data_ids, nb_samples*validation_percentage)
    train_ids = data.produce_train_indices(dataset_indx=data_ids, number_of_samples=nb_samples*training_percentage,
                                           val_indx=val_ids)
    #X_train, y_train, X_test, y_test = data.split_data(X, y, split_ratio=split)
    X_train, y_train, X_val, y_val = data.load_dataset_bit_from_hdf5(train_ids, val_ids, only_train=False)
    X_val = X_val / 255

    print("Building and Compiling model..")
    model = m.get_model(n_outputs=num_categories, input_size=size)

    if cont:
        #model.load_weights_until_layer("pre_trained_weights/latest_model_weights.hdf5", 26)
        model.load_weights("pre_trained_weights/latest_model_weights.hdf5")
    model.compile(optimizer='adam', loss='categorical_crossentropy')

    print("Training..")

    best_performance = np.inf
    for i in range(epochs):
        train_ids = data.produce_train_indices(dataset_indx=data_ids, number_of_samples=15000, val_indx=val_ids)

        X_train, y_train = data.load_dataset_bit_from_hdf5(train_ids, val_ids, only_train=True)

        X_train = X_train / 255
        X_train = data.augment_data(X_train)

        # fit the model on the batches generated by datagen.flow()
        metadata = model.fit(X_train, y_train, validation_data=[X_val, y_val], batch_size=64,
                             nb_epoch=1, verbose=1, shuffle=True, show_accuracy=True, class_weight=None,
                             sample_weight=None)
        current_loss = metadata.history['loss'][-1]
        current_val_loss = metadata.history['val_loss'][-1]
        preds = model.predict_proba(X_val, batch_size=64)
        print("Loss: " + str(current_loss))
        print("Val_loss: " + str(current_val_loss))

        top_3_error = get_top_n_error(preds, y_val, top_k)
        print("Top 3 error: "+str(top_3_error))
        if current_val_loss < best_performance:
            model.save_weights("pre_trained_weights/model_weights.hdf5", overwrite=True)
            best_performance = current_val_loss
            print("Saving weights..")
        model.save_weights("pre_trained_weights/latest_model_weights.hdf5", overwrite=True)



def extract_data(size=256):
    print("Extracting data..")
    X, y = data.extract_data(size=256)

    print("Preprocessing data..")
    X, y, nb_samples, num_categories = data.preprocess_data(X, y, save=True, subtract_mean=True)

    return X, y, nb_samples, num_categories


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--extract_data', dest='extract_data', action='store_const', const=True, default=False)
    parser.add_argument('--run', dest='run', action='store_const', const=True, default=False)
    parser.add_argument('--continue', dest='continue_training', action='store_const', const=True, default=False)
    args = parser.parse_args()

    extract_mode = args.extract_data
    run_mode = args.run
    continue_ = args.continue_training

    if run_mode:
        run(extract=extract_mode, cont=continue_)
    elif extract_mode:
        extract_data()
